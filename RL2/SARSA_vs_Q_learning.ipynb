{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA vs Q-learning_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/islab-ai/AITutorial.git"
      ],
      "metadata": {
        "id": "POptMT-ySUfO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir('/content/AITutorial/RL2')"
      ],
      "metadata": {
        "id": "58un_DS6SeYl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvFnbQ1j8xna",
        "outputId": "f961f27a-2440-49a9-dd46-fe1f5e8d5a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,982 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 993 kB in 1s (855 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.6)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n",
            "\u001b[K     |████████████████████████████████| 538 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599499 sha256=19d6b1ba0f7a0061a97924edad6f6c71e7f1a93017cc3fbb7af044526cfa1ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate, pygame\n",
            "Successfully installed EasyProcess-1.1 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 pygame-2.1.2 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ],
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "# # install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.* pygame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import gym\n",
        "from IPython import display as ipythondisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "display = Display(visible=False, size=(400, 300)) # use False with Xvfb\n",
        "_=display.start()"
      ],
      "metadata": {
        "id": "lmFfCcoR813h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the environment \n",
        "env = gym.make(\"CartPole-v1\")\n",
        "# reset the environment before starting\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "# loop 10 times\n",
        "for i in range(50):\n",
        "    # take a random action\n",
        "    env.step(env.action_space.sample())\n",
        "    # render the game\n",
        "    screen = env.render(mode='rgb_array')\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "# close the environment\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "pOyAFZOZ9n5n",
        "outputId": "d6092661-5dcc-4cf8-ecee-1cb629356c9f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def eps_greedy(Q, s, eps=0.1):\n",
        "    '''\n",
        "    Epsilon greedy policy : 0~1 사이 Epsilon값으로 Action 결정\n",
        "    Epsilon 확률로 Random action 선택,\n",
        "    1-Epsiilon확률로는 최상의 결과를 냈던 Action 선택\n",
        "    '''\n",
        "    if np.random.uniform(0,1) < eps:\n",
        "        # Choose a random action\n",
        "        return np.random.randint(Q.shape[1])\n",
        "    else:\n",
        "        # Choose the action of a greedy policy\n",
        "        return greedy(Q, s)"
      ],
      "metadata": {
        "id": "4clm3sfLAUK1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy(Q, s):\n",
        "    '''\n",
        "    Greedy policy : Q function (Action-state value) 값이 가장 높았던 action 선택\n",
        "    '''\n",
        "    return np.argmax(Q[s])"
      ],
      "metadata": {
        "id": "UBkBtbWfAvcd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(env, Q, num_episodes=100, to_print=False):\n",
        "    '''\n",
        "    Run some episodes to test the policy\n",
        "    '''\n",
        "    tot_rew = []\n",
        "    state = env.reset()\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        done = False\n",
        "        game_rew = 0\n",
        "\n",
        "        while not done:\n",
        "            # select a greedy action\n",
        "            next_state, rew, done, _ = env.step(greedy(Q, state))\n",
        "\n",
        "            state = next_state\n",
        "            game_rew += rew \n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                tot_rew.append(game_rew)\n",
        "\n",
        "    if to_print:\n",
        "        print('Mean score: %.3f of %i games!'%(np.mean(tot_rew), num_episodes))\n",
        "\n",
        "    return np.mean(tot_rew)"
      ],
      "metadata": {
        "id": "nKoDLwvWAxgV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Q_learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n",
        "    nA = env.action_space.n\n",
        "    nS = env.observation_space.n\n",
        "\n",
        "    # Initialize the Q matrix\n",
        "    # Q: matrix nS*nA where each row represent a state and each colums represent a different action\n",
        "    Q = np.zeros((nS, nA))  # 각 State별 선택 가능한 Action을 표현하는 행렬. 추후 각 State에서 Action별 점수를 담을 공간\n",
        "    print(\"초기 Q table: \\n{}\".format(Q))\n",
        "\n",
        "    games_reward = []\n",
        "    test_rewards = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        tot_rew = 0\n",
        "        \n",
        "        # decay the epsilon value until it reaches the threshold of 0.01\n",
        "        if eps > 0.01: # Episode를 반복할 수록 epsilon의 크기를 줄여나가는 기법        \n",
        "            eps -= eps_decay\n",
        "\n",
        "        # loop the main body until the environment stops\n",
        "        while not done:\n",
        "            # eps-greedy policy을 통해서 현재 state에 대한 action 선택\n",
        "            action = eps_greedy(Q, state, eps)\n",
        "\n",
        "            next_state, rew, done, _ = env.step(action) # Take one step in the environment\n",
        "\n",
        "            # Q-learning update the state-action value (get the max Q value for the next state)\n",
        "            # 해당 State에서 action을 진행해보고 받은 next_state와 reward를 활용하여 Q value(action-state value) 업데이트\n",
        "            # next_state에서 가질 수 있는 가장 큰 Q 값을 가져옴\n",
        "            Q[state][action] = Q[state][action] + lr*(rew + gamma*np.max(Q[next_state]) - Q[state][action])\n",
        "\n",
        "            # 다음 state는 환경에서 넘겨준 next_state          \n",
        "            state = next_state\n",
        "            tot_rew += rew\n",
        "            if done:\n",
        "                games_reward.append(tot_rew)\n",
        "\n",
        "        # Test the policy every 300 episodes and print the results\n",
        "        if (ep % 300) == 0:\n",
        "            test_rew = run_episodes(env, Q, 1000)\n",
        "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
        "            test_rewards.append(test_rew)\n",
        "            \n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "iUNFKAVUBCHC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n",
        "    nA = env.action_space.n\n",
        "    nS = env.observation_space.n\n",
        "\n",
        "    # Initialize the Q matrix\n",
        "    # Q: matrix nS*nA where each row represent a state and each colums represent a different action\n",
        "    Q = np.zeros((nS, nA))\n",
        "    games_reward = []\n",
        "    test_rewards = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        tot_rew = 0\n",
        "\n",
        "        # decay the epsilon value until it reaches the threshold of 0.01\n",
        "        if eps > 0.01:\n",
        "            eps -= eps_decay\n",
        "\n",
        "        # 첫 시작 action은 eps_greedy\n",
        "        action = eps_greedy(Q, state, eps) \n",
        "\n",
        "        # loop the main body until the environment stops\n",
        "        while not done:\n",
        "            next_state, rew, done, _ = env.step(action) # Take one step in the environment\n",
        "\n",
        "            # choose the next action (needed for the SARSA update)\n",
        "            next_action = eps_greedy(Q, next_state, eps) \n",
        "            # SARSA update\n",
        "            # next_state에서 현재 policy에 따라 선택할 next_action에 대한 Q-value를 구함\n",
        "            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            tot_rew += rew\n",
        "            if done:\n",
        "                games_reward.append(tot_rew)\n",
        "\n",
        "        # Test the policy every 300 episodes and print the results\n",
        "        if (ep % 300) == 0:\n",
        "            test_rew = run_episodes(env, Q, 1000)\n",
        "            print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n",
        "            test_rewards.append(test_rew)\n",
        "\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "_nZl6UuzBEh7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "    \n",
        "Q_qlearning = Q_learning(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBzcfxBeBFQL",
        "outputId": "f56f9d09-7b56-4bcb-c559-4b0a57e713ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:    0  Eps:0.3990  Rew:-207.2000\n",
            "Episode:  300  Eps:0.0990  Rew:-199.3550\n",
            "Episode:  600  Eps:0.0100  Rew:-178.0340\n",
            "Episode:  900  Eps:0.0100  Rew:-137.0160\n",
            "Episode: 1200  Eps:0.0100  Rew:-87.8180\n",
            "Episode: 1500  Eps:0.0100  Rew:-68.0670\n",
            "Episode: 1800  Eps:0.0100  Rew:-46.4170\n",
            "Episode: 2100  Eps:0.0100  Rew:-22.7670\n",
            "Episode: 2400  Eps:0.0100  Rew:-20.6320\n",
            "Episode: 2700  Eps:0.0100  Rew:2.6060\n",
            "Episode: 3000  Eps:0.0100  Rew:2.2920\n",
            "Episode: 3300  Eps:0.0100  Rew:6.0170\n",
            "Episode: 3600  Eps:0.0100  Rew:6.8730\n",
            "Episode: 3900  Eps:0.0100  Rew:7.9710\n",
            "Episode: 4200  Eps:0.0100  Rew:7.9730\n",
            "Episode: 4500  Eps:0.0100  Rew:7.7440\n",
            "Episode: 4800  Eps:0.0100  Rew:7.7510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_sarsa = SARSA(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNdLP6ynKp0d",
        "outputId": "7b7f09fe-453f-4d08-fb95-97105033a7d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:    0  Eps:0.3990  Rew:-250.2110\n",
            "Episode:  300  Eps:0.0990  Rew:-225.7910\n",
            "Episode:  600  Eps:0.0100  Rew:-181.4820\n",
            "Episode:  900  Eps:0.0100  Rew:-185.2700\n",
            "Episode: 1200  Eps:0.0100  Rew:-119.8860\n",
            "Episode: 1500  Eps:0.0100  Rew:-48.7530\n",
            "Episode: 1800  Eps:0.0100  Rew:-34.2700\n",
            "Episode: 2100  Eps:0.0100  Rew:-42.7040\n",
            "Episode: 2400  Eps:0.0100  Rew:-5.9920\n",
            "Episode: 2700  Eps:0.0100  Rew:-0.2400\n",
            "Episode: 3000  Eps:0.0100  Rew:4.1540\n",
            "Episode: 3300  Eps:0.0100  Rew:4.1200\n",
            "Episode: 3600  Eps:0.0100  Rew:5.6940\n",
            "Episode: 3900  Eps:0.0100  Rew:7.4250\n",
            "Episode: 4200  Eps:0.0100  Rew:7.9170\n",
            "Episode: 4500  Eps:0.0100  Rew:7.9890\n",
            "Episode: 4800  Eps:0.0100  Rew:8.0140\n"
          ]
        }
      ]
    }
  ]
}